{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics\n",
    "=========================================\n",
    "\n",
    "1. One neuron\n",
    "-----------------------------------------\n",
    "The original inspiration of Nerual Networks:  modeling biological neural system.\n",
    "\n",
    "![GitHub Logo](./images/Basics_1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron():\n",
    "    def __init__(self):\n",
    "        self.W \n",
    "        self.b \n",
    "        \n",
    "    def forward(X):\n",
    "    # X: nxd  W: dx1 b:nx1\n",
    "    # y: nx1\n",
    "                                \n",
    "    def backward(y):    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are many different types of neurons, each with different properties.\n",
    "\n",
    "### activation function\n",
    "> * Sigmoid  \n",
    "    $$ \\sigma(x)=\\frac{1}{1+e^{(-x)}} $$  \n",
    "    -- recently rarely used in practice  \n",
    "    -- Cons: saturate and kill gradients; outputs are non zero-centered.  \n",
    "> * Tanh   \n",
    "    $$ tanh(x)=2\\sigma(2x)-1$$    \n",
    "    -- zero-centered  \n",
    "> * ReLU (the Rectified Linear Unit)  \n",
    "    $$ f(x)=\\max(0,x) $$  \n",
    "    -- very popular in the last few years  \n",
    "    -- Cons:  It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions;  can be implemented by simply thresholding a matrix of activations at zero.  \n",
    "    -- Pros: ReLU units can be fragile during training and can “die”.  \n",
    "> * Leaky ReLU   \n",
    "    $$ f(n) =\n",
    "        \\begin{cases} \n",
    "        1,  & x<0 \\\\\n",
    "        \\alpha x+1, &x\\ge 0\n",
    "        \\end{cases}\n",
    "    $$    \n",
    "    -- one attemps to fix the \"dying ReLU\". $\\alpha$ is a small constant.  \n",
    "> * Maxout   \n",
    "    $$ \\max (w_1^Tx+b_1,w_2^T+b_2)$$    \n",
    "\n",
    "TLDR: “What neuron type should I use?” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.\n",
    "    \n",
    "    \n",
    "### linear classifer\n",
    "> * Binary Softmax classifier\n",
    "\n",
    "> * Binary SVM classifier\n",
    "\n",
    "> * Regularization interpretation\n",
    "\n",
    "\n",
    "2. Architectures\n",
    "-----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
